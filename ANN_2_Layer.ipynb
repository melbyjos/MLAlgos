{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Layer Artificial Neural Network\n",
    "### Joseph Melby\n",
    "### 11/26/18\n",
    "\n",
    "#### Given training data: MNIST X train.csv (feature values), MNIST y train.csv (labels)\n",
    "\n",
    "#### Test data: MNIST X test.csv (feature values), MNIST y test.csv (labels) .\n",
    "\n",
    "#### File House feature MNIST description.csv gives a brief introduction to these data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I normalize and prepare the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 784)\n",
      "(500, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joeme\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "def read_dataset(feature_file, label_file):\n",
    "    ''' Read data set in *.csv to data frame in Pandas\n",
    "    and return numpy arrays for features and labels respectively.\n",
    "    \n",
    "    Args:\n",
    "    feature_file: string, path to the CSV file containing features\n",
    "    label_file: string, path to the CSV file containing labels\n",
    "    \n",
    "    Returns:\n",
    "    numpy array: features, shape=(num_samples, num_features)\n",
    "    numpy array: labels, shape=(num_samples, 1)\n",
    "    '''\n",
    "    df_X = pd.read_csv(feature_file)\n",
    "    df_y = pd.read_csv(label_file)\n",
    "    X = df_X.values # convert values in dataframe to numpy array (features)\n",
    "    y = df_y.values # convert values in dataframe to numpy array (label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train = read_dataset('MNIST_X_train.csv', 'MNIST_y_train.csv')\n",
    "X_test, y_test = read_dataset('MNIST_X_test.csv', 'MNIST_y_test.csv')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "def plot_digit(feature_vector): \n",
    "    ''' Plot a given feature vector as a digit image in grayscale\n",
    "    \n",
    "    Args:\n",
    "    feature_vector: numpy array, shape=(num_features, )\n",
    "    '''\n",
    "    plt.gray() \n",
    "    plt.matshow(feature_vector.reshape(28,28))\n",
    "    plt.show() \n",
    "\n",
    "def normalize_features(X_train, X_test):\n",
    "    ''' Normalize features in training and test data using StandardScaler\n",
    "    \n",
    "    Args:\n",
    "    X_train: numpy array, shape=(num_train_samples, num_features)\n",
    "        Training features\n",
    "    X_test: numpy array, shape=(num_test_samples, num_features)\n",
    "        Test features\n",
    "        \n",
    "    Returns:\n",
    "    numpy array: normalized training features, shape=(num_train_samples, num_features)\n",
    "    numpy array: normalized test features, shape=(num_test_samples, num_features)\n",
    "    '''\n",
    "    from sklearn.preprocessing import StandardScaler #import libaray\n",
    "    scaler = StandardScaler() # call an object function\n",
    "    scaler.fit(X_train) # calculate mean, std in X_train\n",
    "    X_train_norm = scaler.transform(X_train) # apply normalization on X_train\n",
    "    X_test_norm = scaler.transform(X_test) # we use the same normalization on X_test\n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "\n",
    "X_train_norm, X_test_norm = normalize_features(X_train, X_test)\n",
    "\n",
    "\n",
    "def one_hot_encoder(y_train, y_test):\n",
    "    ''' Convert label to a vector under one-hot-code fashion\n",
    "    \n",
    "    Args:\n",
    "    y_train: numpy array, shape=(num_train_samples, 1)\n",
    "        Training labels\n",
    "    y_test: numpy array, shape=(num_test_samples, 1)\n",
    "        Test labels\n",
    "        \n",
    "    Returns:\n",
    "    numpy array: one-hot encoded training labels, shape=(num_train_samples, num_classes)\n",
    "    numpy array: one-hot encoded test labels, shape=(num_test_samples, num_classes)\n",
    "    '''\n",
    "    from sklearn import preprocessing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(y_train)\n",
    "    y_train_ohe = lb.transform(y_train)\n",
    "    y_test_ohe = lb.transform(y_test)\n",
    "    return y_train_ohe, y_test_ohe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the two layer ANN class, as well as necessary supporting functions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: I was not able to figure out the regularization portion of the assignment. I was planning to use a regularized loss function. However, I have not figured out how to implement this. I believe we are supposed to instead use an L2-norm regularization loss, but I did not have time to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twolayer_NN:\n",
    "    \"\"\"\n",
    "    A class for a two-layer neural network.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    X : numpy array\n",
    "        Input data of shape (number of samples, number of features)\n",
    "    y : numpy array\n",
    "        Target data of shape (number of samples, number of output units)\n",
    "    hidden_layer_nn : int\n",
    "        Number of neurons in the hidden layer\n",
    "    lr : float\n",
    "        Learning rate for gradient descent\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    __init__(self, X, y, hidden_layer_nn=100, lr=0.01):\n",
    "        Initializes the weights and biases of the network and sets the attributes.\n",
    "\n",
    "    feed_forward(self):\n",
    "        Computes the forward pass of the neural network.\n",
    "\n",
    "    back_propagation(self):\n",
    "        Computes the gradients of the neural network using backpropagation and updates the weights and biases using gradient descent.\n",
    "\n",
    "    cross_entropy_loss(self):\n",
    "        Computes the cross-entropy loss of the neural network.\n",
    "\n",
    "    predict(self, X_test):\n",
    "        Predicts the class labels of new data using the trained neural network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, hidden_layer_nn=100, lr=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases of the network and sets the attributes.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            Input data of shape (number of samples, number of features)\n",
    "        y : numpy array\n",
    "            Target data of shape (number of samples, number of output units)\n",
    "        hidden_layer_nn : int, optional\n",
    "            Number of neurons in the hidden layer, by default 100\n",
    "        lr : float, optional\n",
    "            Learning rate for gradient descent, by default 0.01\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.hidden_layer_nn = hidden_layer_nn\n",
    "        self.lr = lr\n",
    "        #Initialize the weights\n",
    "        self.nn = X.shape[1] #number of neurons in input layer/ number of features / second coordinate 64\n",
    "        self.W1 = np.random.randn(self.nn, hidden_layer_nn) / np.sqrt(self.nn) #dim(W1) = nn * hidden_layer_nn\n",
    "        self.b1 = np.zeros((1, hidden_layer_nn))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_layer_nn, hidden_layer_nn) / np.sqrt(self.nn) #dim(W2) = nn * hidden_layer_nn\n",
    "        self.b2 = np.zeros((1, hidden_layer_nn))\n",
    "        \n",
    "        self.output_layer_nn = y.shape[1]\n",
    "        self.W3 = np.random.randn(hidden_layer_nn, self.output_layer_nn) / np.sqrt(hidden_layer_nn)\n",
    "        self.b3 = np.zeros((1, self.output_layer_nn))\n",
    "    \n",
    "    def feed_forward(self):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the neural network to compute the predicted output.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        y_hat : numpy.ndarray\n",
    "            A 1D array of shape (n_classes,) representing the predicted class probabilities.\n",
    "        \"\"\"\n",
    "        #hidden layer 1\n",
    "        ## z_1 = xW_1 + b_1\n",
    "        self.z1 = np.dot(self.X, self.W1) + self.b1\n",
    "        #self.f1 = np.copy(self.z1)\n",
    "        self.z1[self.z1 < 0] = 0\n",
    "        self.f1 = self.z1\n",
    "        #hidden layer 2\n",
    "        self.z2 = np.dot(self.f1, self.W2) + self.b2\n",
    "        #self.f2 = np.copy(self.z2)\n",
    "        self.z2[self.z2 < 0] = 0\n",
    "        self.f2 = self.z2\n",
    "        \n",
    "        # Output layer\n",
    "        self.z3 = np.dot(self.f2, self.W3) + self.b3\n",
    "        self.y_hat = softmax(self.z3)\n",
    "        \n",
    "    def back_propagation(self):\n",
    "        \"\"\"\n",
    "        Back propagation step of the neural network. Calculates the gradients of the weights and biases\n",
    "        using the chain rule of derivatives and updates the weights and biases using the gradient descent algorithm.\n",
    "        \"\"\"\n",
    "        d3 = self.y_hat - self.y\n",
    "        dW3 = np.dot(self.f2.T, d3)\n",
    "        db3 = np.sum(d3, axis = 0, keepdims = True)\n",
    "        # axis = 0 means sum along column/vertical\n",
    "        #d2 = self.f1*np.dot(d3, vdReLu(self.z2).T)\n",
    "        self.z2[self.z2 > 0] = 0.001\n",
    "        d2 = np.dot(d3, self.W3.T)*self.z2\n",
    "        dW2 = np.dot(self.f1.T, d2)\n",
    "        db2 = np.sum(d2, axis = 0, keepdims = True)\n",
    "        \n",
    "        #d1 = np.dot(d2, vdReLu(self.z1))*self.W2\n",
    "        self.z1[self.z1 > 0] = 0.0001\n",
    "        d1 = np.dot(d2,self.W2.T)*self.z1\n",
    "        dW1 = np.dot(self.X.T, d1)\n",
    "       \n",
    "        db1 = np.sum(d1, axis = 0, keepdims = True)\n",
    "        \n",
    "        #Update Gradient Descent\n",
    "        self.W1 = self.W1 - self.lr*dW1\n",
    "        self.b1 = self.b1 - self.lr*db1\n",
    "        \n",
    "        self.W2 = self.W2 - self.lr*dW2\n",
    "        self.b2 = self.b2 - self.lr*db2\n",
    "        \n",
    "        self.W3 = self.W3 - self.lr*dW3\n",
    "        self.b3 = self.b3 - self.lr*db3\n",
    "\n",
    "    def cross_entropy_loss(self):\n",
    "        self.feed_forward() #update self.y_hat\n",
    "        self.loss = -np.sum(self.y*np.log(self.y_hat + 1e-10))\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for the given input data X_test.\n",
    "\n",
    "        Args:\n",
    "            X_test (numpy.ndarray): A numpy array of shape (num_test_samples, num_features) containing the test input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: An array of shape (num_test_samples,) containing the predicted class labels for the input data.\n",
    "        \"\"\"\n",
    "        z1 = np.dot(X_test, self.W1) + self.b1\n",
    "        z1[z1 < 0] = 0\n",
    "        f1 = z1\n",
    "        \n",
    "        z2 = np.dot(f1, self.W2) + self.b2\n",
    "        z2[z2 < 0] = 0\n",
    "        f2 = z2\n",
    "        \n",
    "        # Output layer\n",
    "        z3 = np.dot(f2, self.W3) + self.b3\n",
    "        y_hat_test = softmax(z3)\n",
    "        \n",
    "        labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        num_test_samples = X_test.shape[0]\n",
    "        # find which index gives us the highest probability\n",
    "        ypred = np.zeros(num_test_samples, dtype=int) \n",
    "        for i in range(num_test_samples):\n",
    "            ypred[i] = labels[np.argmax(y_hat_test[i,:])]\n",
    "        return ypred\n",
    "\n",
    "    \n",
    "    \n",
    "# def ReLu(t):\n",
    "#     if t<0:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return t\n",
    "# vReLu = np.vectorize(ReLu)\n",
    "    \n",
    "# def dReLu(t):\n",
    "#     if t<0:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return 1\n",
    "# vdReLu = np.vectorize(dReLu)\n",
    "\n",
    "def accuracy(ypred, yexact):\n",
    "    \"\"\"\n",
    "    Computes the accuracy score given predicted labels and true labels.\n",
    "    \n",
    "    Parameters:\n",
    "        ypred (numpy.ndarray): 1D array of predicted labels.\n",
    "        yexact (numpy.ndarray): 1D array of true labels.\n",
    "    \n",
    "    Returns:\n",
    "        float: The accuracy score.\n",
    "    \"\"\"\n",
    "    p = np.array(ypred == yexact, dtype=int)\n",
    "    return np.sum(p) / float(len(yexact))\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Computes the softmax function for an input array of logits.\n",
    "    \n",
    "    Parameters:\n",
    "        z (numpy.ndarray): The input array of logits.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The softmax scores for each input logit.\n",
    "    \"\"\"\n",
    "    exp_value = np.exp(z - np.amax(z, axis=1, keepdims=True))\n",
    "    softmax_scores = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "    return softmax_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1200 iterations seems to give reasonable accuracy in order to compare learning rates and number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with a learning rate of 0.001:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for 20 neurons with learning rate of 0.001 and 1200 epochs:\n",
      "epoch = 300, current loss = 3513.90022\n",
      "epoch = 600, current loss = 2954.01716\n",
      "epoch = 900, current loss = 2729.03528\n",
      "epoch = 1200, current loss = 2623.42821\n",
      "Accuracy of our model  0.546\n",
      "Model for 50 neurons with learning rate of 0.001 and 1200 epochs:\n",
      "epoch = 300, current loss = 2257.91672\n",
      "epoch = 600, current loss = 1723.27663\n",
      "epoch = 900, current loss = 1502.87426\n",
      "epoch = 1200, current loss = 1376.44580\n",
      "Accuracy of our model  0.716\n",
      "Model for 100 neurons with learning rate of 0.001 and 1200 epochs:\n",
      "epoch = 300, current loss = 1295.58845\n",
      "epoch = 600, current loss = 910.65087\n",
      "epoch = 900, current loss = 720.35496\n",
      "epoch = 1200, current loss = 595.17689\n",
      "Accuracy of our model  0.824\n",
      "Model for 150 neurons with learning rate of 0.001 and 1200 epochs:\n",
      "epoch = 300, current loss = 951.22536\n",
      "epoch = 600, current loss = 624.86180\n",
      "epoch = 900, current loss = 458.22548\n",
      "epoch = 1200, current loss = 348.33651\n",
      "Accuracy of our model  0.836\n",
      "Model for 200 neurons with learning rate of 0.001 and 1200 epochs:\n",
      "epoch = 300, current loss = 741.59827\n",
      "epoch = 600, current loss = 463.54513\n",
      "epoch = 900, current loss = 320.59060\n",
      "epoch = 1200, current loss = 230.40799\n",
      "Accuracy of our model  0.832\n"
     ]
    }
   ],
   "source": [
    "for n in [20,50,100,150,200]:\n",
    "    #initialize a class \n",
    "    myNN2 = twolayer_NN(X_train_norm, y_train_ohe, hidden_layer_nn = n, lr = 0.001)\n",
    "    epoch_num = 1200 #num of iterations\n",
    "    print('Model for %d neurons with learning rate of 0.001 and %d epochs:' % (n, epoch_num))\n",
    "    for i in range(epoch_num):\n",
    "        myNN2.feed_forward()\n",
    "        myNN2.back_propagation()\n",
    "        myNN2.cross_entropy_loss()\n",
    "        current_loss = myNN2.loss\n",
    "        if ((i+1)%300 == 0):\n",
    "            print('epoch = %d, current loss = %.5f' % (i+1, myNN2.loss))\n",
    "\n",
    "    # Validate trained model against test data:\n",
    "\n",
    "    ypred = myNN2.predict(X_test_norm)\n",
    "    print('Accuracy of our model ', accuracy(ypred, y_test.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with a learning rate of 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for 20 neurons with learning rate of 0.01 and 1200 epochs:\n",
      "epoch = 100, current loss = 3268.34392\n",
      "epoch = 200, current loss = 2868.33519\n",
      "epoch = 300, current loss = 2639.81562\n",
      "epoch = 400, current loss = 2453.46863\n",
      "epoch = 500, current loss = 2384.48748\n",
      "epoch = 600, current loss = 2238.85384\n",
      "epoch = 700, current loss = 2207.90627\n",
      "epoch = 800, current loss = 2124.08280\n",
      "epoch = 900, current loss = 2084.17572\n",
      "epoch = 1000, current loss = 2052.33707\n",
      "epoch = 1100, current loss = 2030.98488\n",
      "epoch = 1200, current loss = 2010.75838\n",
      "Accuracy of our model  0.528\n",
      "Model for 50 neurons with learning rate of 0.01 and 1200 epochs:\n",
      "epoch = 100, current loss = 1682.46125\n",
      "epoch = 200, current loss = 1183.09130\n",
      "epoch = 300, current loss = 911.16276\n",
      "epoch = 400, current loss = 714.70906\n",
      "epoch = 500, current loss = 565.70943\n",
      "epoch = 600, current loss = 452.00085\n",
      "epoch = 700, current loss = 362.15363\n",
      "epoch = 800, current loss = 292.65695\n",
      "epoch = 900, current loss = 237.87700\n",
      "epoch = 1000, current loss = 195.66972\n",
      "epoch = 1100, current loss = 162.88408\n",
      "epoch = 1200, current loss = 137.32757\n",
      "Accuracy of our model  0.738\n",
      "Model for 100 neurons with learning rate of 0.01 and 1200 epochs:\n",
      "epoch = 100, current loss = 779.62782\n",
      "epoch = 200, current loss = 393.97879\n",
      "epoch = 300, current loss = 227.78897\n",
      "epoch = 400, current loss = 143.66417\n",
      "epoch = 500, current loss = 97.28586\n",
      "epoch = 600, current loss = 70.15725\n",
      "epoch = 700, current loss = 53.31615\n",
      "epoch = 800, current loss = 42.22134\n",
      "epoch = 900, current loss = 34.54944\n",
      "epoch = 1000, current loss = 29.01230\n",
      "epoch = 1100, current loss = 24.85047\n",
      "epoch = 1200, current loss = 21.63684\n",
      "Accuracy of our model  0.8\n",
      "Model for 150 neurons with learning rate of 0.01 and 1200 epochs:\n",
      "epoch = 100, current loss = 414.88885\n",
      "epoch = 200, current loss = 165.07710\n",
      "epoch = 300, current loss = 83.21920\n",
      "epoch = 400, current loss = 50.87062\n",
      "epoch = 500, current loss = 35.17297\n",
      "epoch = 600, current loss = 26.31815\n",
      "epoch = 700, current loss = 20.76041\n",
      "epoch = 800, current loss = 16.99919\n",
      "epoch = 900, current loss = 14.30789\n",
      "epoch = 1000, current loss = 12.30054\n",
      "epoch = 1100, current loss = 10.75113\n",
      "epoch = 1200, current loss = 9.52250\n",
      "Accuracy of our model  0.854\n",
      "Model for 200 neurons with learning rate of 0.01 and 1200 epochs:\n",
      "epoch = 100, current loss = 301.32413\n",
      "epoch = 200, current loss = 106.82923\n",
      "epoch = 300, current loss = 53.50729\n",
      "epoch = 400, current loss = 33.17492\n",
      "epoch = 500, current loss = 23.29883\n",
      "epoch = 600, current loss = 17.65981\n",
      "epoch = 700, current loss = 14.06891\n",
      "epoch = 800, current loss = 11.61284\n",
      "epoch = 900, current loss = 9.83920\n",
      "epoch = 1000, current loss = 8.50635\n",
      "epoch = 1100, current loss = 7.47119\n",
      "epoch = 1200, current loss = 6.64605\n",
      "Accuracy of our model  0.864\n"
     ]
    }
   ],
   "source": [
    "for n in [20,50,100,150,200]:\n",
    "    #initialize a class \n",
    "    myNN2 = twolayer_NN(X_train_norm, y_train_ohe, hidden_layer_nn = n, lr = 0.01)\n",
    "    epoch_num = 1200 #num of iterations\n",
    "    print('Model for %d neurons with learning rate of 0.01 and %d epochs:' % (n, epoch_num))\n",
    "    for i in range(epoch_num):\n",
    "        myNN2.feed_forward()\n",
    "        myNN2.back_propagation()\n",
    "        myNN2.cross_entropy_loss()\n",
    "        current_loss = myNN2.loss\n",
    "        if ((i+1)%100 == 0):\n",
    "            print('epoch = %d, current loss = %.5f' % (i+1, myNN2.loss))\n",
    "\n",
    "    # Validate trained model against test data:\n",
    "\n",
    "    ypred = myNN2.predict(X_test_norm)\n",
    "    print('Accuracy of our model ', accuracy(ypred, y_test.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with a learning rate of 0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for 20 neurons with learning rate of 0.001 and 1200 epochs:\n",
      "epoch = 100, current loss = 38065.08830\n",
      "epoch = 200, current loss = 37897.81498\n",
      "epoch = 300, current loss = 37956.62633\n",
      "epoch = 400, current loss = 37361.22751\n",
      "epoch = 500, current loss = 41860.23905\n",
      "epoch = 600, current loss = 41123.61540\n",
      "epoch = 700, current loss = 40617.59488\n",
      "epoch = 800, current loss = 38523.54915\n",
      "epoch = 900, current loss = 38828.04360\n",
      "epoch = 1000, current loss = 38047.07759\n",
      "epoch = 1100, current loss = 41560.27138\n",
      "epoch = 1200, current loss = 37818.42181\n",
      "Accuracy of our model  0.072\n",
      "Model for 50 neurons with learning rate of 0.001 and 1200 epochs:\n",
      "epoch = 100, current loss = 41804.34724\n",
      "epoch = 200, current loss = 36768.42924\n",
      "epoch = 300, current loss = 40657.66685\n",
      "epoch = 400, current loss = 41207.46372\n",
      "epoch = 500, current loss = 33279.35753\n",
      "epoch = 600, current loss = 39366.02957\n",
      "epoch = 700, current loss = 35666.38923\n",
      "epoch = 800, current loss = 34866.54790\n",
      "epoch = 900, current loss = 37813.16171\n",
      "epoch = 1000, current loss = 37255.78989\n",
      "epoch = 1100, current loss = 41814.55904\n",
      "epoch = 1200, current loss = 36294.42505\n",
      "Accuracy of our model  0.072\n",
      "Model for 100 neurons with learning rate of 0.001 and 1200 epochs:\n",
      "epoch = 100, current loss = 40617.57988\n",
      "epoch = 200, current loss = 34788.46637\n",
      "epoch = 300, current loss = 40921.17740\n",
      "epoch = 400, current loss = 36160.49830\n",
      "epoch = 500, current loss = 41170.22146\n",
      "epoch = 600, current loss = 41146.33899\n",
      "epoch = 700, current loss = 36807.15391\n",
      "epoch = 800, current loss = 41860.99673\n",
      "epoch = 900, current loss = 33430.55008\n",
      "epoch = 1000, current loss = 40617.60038\n",
      "epoch = 1100, current loss = 41284.21590\n",
      "epoch = 1200, current loss = 39613.09011\n",
      "Accuracy of our model  0.102\n",
      "Model for 150 neurons with learning rate of 0.001 and 1200 epochs:\n",
      "epoch = 100, current loss = 39133.11226\n",
      "epoch = 200, current loss = 33634.34769\n",
      "epoch = 300, current loss = 41490.20043\n",
      "epoch = 400, current loss = 36888.03071\n",
      "epoch = 500, current loss = 32868.20731\n",
      "epoch = 600, current loss = 38123.79283\n",
      "epoch = 700, current loss = 36467.84620\n",
      "epoch = 800, current loss = 38438.46931\n",
      "epoch = 900, current loss = 33989.97479\n",
      "epoch = 1000, current loss = 31567.54947\n",
      "epoch = 1100, current loss = 35787.88220\n",
      "epoch = 1200, current loss = 37696.94503\n",
      "Accuracy of our model  0.094\n",
      "Model for 200 neurons with learning rate of 0.001 and 1200 epochs:\n",
      "epoch = 100, current loss = 32604.09301\n",
      "epoch = 200, current loss = 41813.74989\n",
      "epoch = 300, current loss = 35049.03673\n",
      "epoch = 400, current loss = 40708.30793\n",
      "epoch = 500, current loss = 40180.04154\n",
      "epoch = 600, current loss = 41860.98550\n",
      "epoch = 700, current loss = 40945.87057\n",
      "epoch = 800, current loss = 37182.83441\n",
      "epoch = 900, current loss = 39083.55710\n",
      "epoch = 1000, current loss = 36731.68399\n",
      "epoch = 1100, current loss = 40883.65599\n",
      "epoch = 1200, current loss = 37484.47410\n",
      "Accuracy of our model  0.108\n"
     ]
    }
   ],
   "source": [
    "for n in [20,50,100,150,200]:\n",
    "    #initialize a class \n",
    "    myNN2 = twolayer_NN(X_train_norm, y_train_ohe, hidden_layer_nn = n, lr = 0.1)\n",
    "    epoch_num = 1200 #num of iterations\n",
    "    print('Model for %d neurons with learning rate of 0.001 and %d epochs:' % (n, epoch_num))\n",
    "    for i in range(epoch_num):\n",
    "        myNN2.feed_forward()\n",
    "        myNN2.back_propagation()\n",
    "        myNN2.cross_entropy_loss()\n",
    "        current_loss = myNN2.loss\n",
    "        if ((i+1)%100 == 0):\n",
    "            print('epoch = %d, current loss = %.5f' % (i+1, myNN2.loss))\n",
    "\n",
    "    # Validate trained model against test data:\n",
    "\n",
    "    ypred = myNN2.predict(X_test_norm)\n",
    "    print('Accuracy of our model ', accuracy(ypred, y_test.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above tests, it seems that a learning rate of 0.01, 1200 epochs, and 200 neurons produces a model with the highest accuracy so far. Below is one final test to see if this changes for more neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for 250 neurons with learning rate of 0.01 and 1200 epochs:\n",
      "epoch = 600, current loss = 13.40052\n",
      "epoch = 1200, current loss = 5.24166\n",
      "Accuracy of our model  0.86\n",
      "Model for 300 neurons with learning rate of 0.01 and 1200 epochs:\n",
      "epoch = 600, current loss = 11.88643\n",
      "epoch = 1200, current loss = 4.65041\n",
      "Accuracy of our model  0.866\n"
     ]
    }
   ],
   "source": [
    "for n in [250, 300]:\n",
    "    #initialize a class \n",
    "    myNN2 = twolayer_NN(X_train_norm, y_train_ohe, hidden_layer_nn = n, lr = 0.01)\n",
    "    epoch_num = 1200 #num of iterations\n",
    "    print('Model for %d neurons with learning rate of 0.01 and %d epochs:' % (n, epoch_num))\n",
    "    for i in range(epoch_num):\n",
    "        myNN2.feed_forward()\n",
    "        myNN2.back_propagation()\n",
    "        myNN2.cross_entropy_loss()\n",
    "        current_loss = myNN2.loss\n",
    "        if ((i+1)%600 == 0):\n",
    "            print('epoch = %d, current loss = %.5f' % (i+1, myNN2.loss))\n",
    "\n",
    "    # Validate trained model against test data:\n",
    "\n",
    "    ypred = myNN2.predict(X_test_norm)\n",
    "    print('Accuracy of our model ', accuracy(ypred, y_test.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With 250 neurons, we are losing some accuracy, but the loss is also lower. This indicates overfitting of the data. 300 neurons results in a small increase in accuracy and a decrease in loss. Given this, it seems like this model is ideal for roughly 200 or 300 neurons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc0da0bd7c01680fa8348b7a26d6a24694e547ab16f98204be81d32b3a15c474"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
