{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "## Regularized Support Vector Machine\n",
        "### Joseph Melby\n",
        "### 11/02/18\n",
        "\n",
        "#### Given training data: MNIST X train.csv (feature values), MNIST y train.csv (labels)\n",
        "\n",
        "#### Test data: MNIST X test.csv (feature values), MNIST y test.csv (labels) .\n",
        "\n",
        "#### File House feature MNIST description.csv gives a brief introduction to these data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def read_dataset(feature_file:str, label_file:str) -> tuple:\n",
        "    \"\"\"Reads data set in *.csv to data frame in Pandas.\n",
        "\n",
        "    Args:\n",
        "    feature_file (str): File path to input features\n",
        "    label_file (str): File path to input labels\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple of numpy arrays (features and labels)\n",
        "    \"\"\"\n",
        "    df_X = pd.read_csv(feature_file)\n",
        "    df_y = pd.read_csv(label_file)\n",
        "    X = df_X.values # convert values in dataframe to numpy array (features)\n",
        "    y = df_y.values # convert values in dataframe to numpy array (label)\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = read_dataset('MNIST_X_train.csv', 'MNIST_y_train.csv')\n",
        "X_test, y_test = read_dataset('MNIST_X_test.csv', 'MNIST_y_test.csv')\n",
        "\n",
        "def plot_digit(feature_vector):\n",
        "    \"\"\"Plots a given digit.\n",
        "\n",
        "    Args:\n",
        "    feature_vector (numpy array): A vector representing a single digit\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    plt.gray() \n",
        "    plt.matshow(feature_vector.reshape(8,8))\n",
        "    plt.show() \n",
        "\n",
        "print('Label ', y_train[0])\n",
        "\n",
        "def normalize_features(X_train:np.ndarray, X_test:np.ndarray) -> tuple:\n",
        "    \"\"\"Normalizes features using StandardScaler from sklearn.\n",
        "\n",
        "    Args:\n",
        "    X_train (numpy array): Training data features\n",
        "    X_test (numpy array): Testing data features\n",
        "\n",
        "    Returns:\n",
        "    tuple: Normalized features for training and testing data\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler() # call an object function\n",
        "    scaler.fit(X_train) # calculate mean, std in X_train\n",
        "    X_train_norm = scaler.transform(X_train) # apply normalization on X_train\n",
        "    X_test_norm = scaler.transform(X_test) # we use the same normalization on X_test\n",
        "    return X_train_norm, X_test_norm\n",
        "\n",
        "X_train_norm, X_test_norm = normalize_features(X_train, X_test)\n",
        "\n",
        "def one_hot_encoder(y_train:np.ndarray, y_test:np.ndarray) -> tuple:\n",
        "    \"\"\"One-hot encodes the given labels.\n",
        "\n",
        "    Args:\n",
        "    y_train (numpy array): Training data labels\n",
        "    y_test (numpy array): Testing data labels\n",
        "\n",
        "    Returns:\n",
        "    tuple: One-hot encoded labels for training and testing data\n",
        "    \"\"\"\n",
        "    trainneg = np.ones(shape=(len(y_train),10))\n",
        "    train = -1*trainneg\n",
        "    testneg = np.ones(shape=(len(y_train),10))\n",
        "    test = -1*testneg\n",
        "    for i in range(len(y_train)):\n",
        "        trainval = y_train[i]\n",
        "        train[i,trainval] = 1\n",
        "        testval = y_train[i]\n",
        "        test[i,testval] = 1\n",
        "        y_train_ohe = train\n",
        "        y_test_ohe = test\n",
        "    return y_train_ohe, y_test_ohe\n",
        "\n",
        "y_train_ohe, y_test_ohe = one_hot_encoder(y_train, y_test)\n",
        "print(y_train_ohe[0])\n",
        "print(y_train_ohe[1])\n",
        "print(y_test_ohe[0])\n",
        "\n",
        "\n",
        "def predictor(X, c, b):\n",
        "    \"\"\"\n",
        "    Computes the prediction for the input data based on the hyperplane equation.\n",
        "    \n",
        "    Args:\n",
        "        X (numpy.ndarray): Input data of shape (n_samples, n_features).\n",
        "        c (numpy.ndarray): Coefficients of the hyperplane of shape (n_features,).\n",
        "        b (float): Intercept of the hyperplane.\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: Prediction of the input data based on the hyperplane equation of shape (n_samples,).\n",
        "    \"\"\"\n",
        "    return X.dot(c) + b\n",
        "\n",
        "\n",
        "def loss(X, y_train, c, b, reg):\n",
        "    \"\"\"\n",
        "    Computes the loss function for SVM.\n",
        "    \n",
        "    Parameters:\n",
        "        X (numpy array): Input data\n",
        "        y_train (numpy array): Target labels\n",
        "        c (numpy array): Coefficients\n",
        "        b (float): Intercept\n",
        "        reg (float): Regularization parameter\n",
        "        \n",
        "    Returns:\n",
        "        float: Value of the loss function\n",
        "    \"\"\"\n",
        "    hingeloss = np.zeros(shape=(len(y_train)))\n",
        "    for i in range(len(y_train)):\n",
        "        hingeloss[i]= np.max([0, 1-y_train[i]*predictor(X,c,b)[i]])\n",
        "    return np.linalg.norm(c) + reg*sum(hingeloss)\n",
        "\n",
        "\n",
        "def hinge_gradient(X, y_train, c, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient of the hinge loss function.\n",
        "    \n",
        "    Parameters:\n",
        "        X (numpy array): Input data\n",
        "        y_train (numpy array): Target labels\n",
        "        c (numpy array): Coefficients\n",
        "        b (float): Intercept\n",
        "        \n",
        "    Returns:\n",
        "        numpy array: Gradient of the hinge loss function\n",
        "    \"\"\"\n",
        "    hingegrad = np.zeros(shape=(len(c)+1))\n",
        "    for i in range(len(y_train)):\n",
        "        if 1-y_train[i]*predictor(X,c,b)[i] > 0:\n",
        "            hingegrad[0] += -y_train[i]\n",
        "            for j in range(len(c)):\n",
        "                hingegrad[j+1] = -y_train[i]*X[i,j]\n",
        "    return hingegrad\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, c, b, reg, epochs=1000, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to optimize the SVM objective function.\n",
        "    \n",
        "    Parameters:\n",
        "        X (numpy array): Input data\n",
        "        y (numpy array): Target labels\n",
        "        c (numpy array): Coefficients\n",
        "        b (float): Intercept\n",
        "        reg (float): Regularization parameter\n",
        "        epochs (int): Number of epochs for training\n",
        "        learning_rate (float): Learning rate for gradient descent\n",
        "        \n",
        "    Returns:\n",
        "        numpy array: Optimized coefficients\n",
        "        float: Optimized intercept\n",
        "        list: Loss history during training\n",
        "    \"\"\"\n",
        "    loss_history = [0]*epochs\n",
        "    normgrad = np.zeros(shape=(len(c)+1))\n",
        "    for i in range(len(c)):\n",
        "        normgrad[i+1] = c[i]*(np.linalg.norm(c))**(-1/2)\n",
        "    hingegrad = hinge_gradient(X, y_train, c, b)\n",
        "    for epoch in range(epochs):\n",
        "        loss_history[epoch] = loss(X, y, c, b, reg).ravel()\n",
        "        gradient = normgrad + hingegrad\n",
        "        b = b - learning_rate*gradient[0]\n",
        "        c = c - learning_rate*gradient[1:]\n",
        "    return c, b, loss_history\n",
        "\n",
        "\n",
        "def log_reg_binary_train(X_train, y_train):  \n",
        "    \"\"\"\n",
        "    Trains a binary logistic regression model.\n",
        "    \n",
        "    Parameters:\n",
        "        X_train (numpy array): Input data\n",
        "        y_train (numpy array): Binary target labels\n",
        "        \n",
        "    Returns:\n",
        "        numpy array: Optimized coefficients of the logistic model\n",
        "    \"\"\"\n",
        "    coeffs_0 = np.zeros((X_train_norm.shape[1], 1))\n",
        "    b_0 = 0\n",
        "    coeffs_grad, b_grad, history_loss = gradient_descent(X_train, y_train, coeffs_0, b_0, 100, epochs=100, learning_rate=0.1)\n",
        "    return coeffs_grad\n",
        "\n",
        "def log_reg_OVR_train(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a set of binary logistic regression models using the \"One-vs-Rest\" approach.\n",
        "\n",
        "    Args:\n",
        "    - X_train (ndarray): Input data with shape (n_samples, n_features).\n",
        "    - y_train (ndarray): Target labels with shape (n_samples, n_classes).\n",
        "\n",
        "    Returns:\n",
        "    - weights_list (list): A list containing the weights of each trained model.\n",
        "    \"\"\"\n",
        "    weights_list = []\n",
        "    for i in range(y_train.shape[1]):\n",
        "        y_train_one_column = y_train[:, i]\n",
        "        weights_one_column = log_reg_binary_train(X_train, y_train_one_column)\n",
        "        weights_list.append(weights_one_column)\n",
        "    return weights_list\n",
        "\n",
        "def prediction(weights_list, X_test):\n",
        "    \"\"\"\n",
        "    Computes the predicted labels for a given set of test samples.\n",
        "\n",
        "    Args:\n",
        "    - weights_list (list): A list of weights of the trained binary logistic regression models.\n",
        "    - X_test (ndarray): Test data with shape (n_samples, n_features).\n",
        "\n",
        "    Returns:\n",
        "    - ypred (ndarray): Predicted labels for each test sample with shape (n_samples,).\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    for weights in weights_list:\n",
        "        decision_one_column = predictor(X_test, weights)\n",
        "        if i == 0:\n",
        "            decision_matrix = decision_one_column\n",
        "        else:\n",
        "            decision_matrix = np.concatenate((decision_matrix, decision_one_column), axis=1)\n",
        "        i += 1\n",
        "    labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    num_test_samples = X_test.shape[0]\n",
        "    ypred = np.zeros(num_test_samples, dtype=int)\n",
        "    for i in range(num_test_samples):\n",
        "        ypred[i] = labels[np.argmax(decision_matrix[i, :])]\n",
        "    return ypred\n",
        "\n",
        "weights_list = log_reg_OVR_train(X_train_norm, y_train_ohe)\n",
        "index = 20    \n",
        "plot_digit(X_test[index])\n",
        "ypred = prediction(weights_list, X_test_norm[index:index+1])\n",
        "print(ypred)\n",
        "\n",
        "def accuracy(ypred, yexact):\n",
        "    \"\"\"\n",
        "    Computes the accuracy of the predicted labels.\n",
        "\n",
        "    Args:\n",
        "    - ypred (ndarray): Predicted labels with shape (n_samples,).\n",
        "    - yexact (ndarray): True labels with shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "    - accuracy (float): The accuracy of the predicted labels.\n",
        "    \"\"\"\n",
        "    p = np.array(ypred == yexact, dtype=int)\n",
        "    return np.sum(p) / float(len(yexact))\n",
        "\n",
        "ypred = prediction(weights_list, X_test_norm)\n",
        "print('Accuracy of our model ', accuracy(ypred, y_test.ravel()))\n",
        "\n",
        "\n",
        "# Stochastic GD (SGD)\n",
        "def SGD(X, y, c, epochs=1000, learning_rate=0.00, batch_size=10):\n",
        "    \"\"\"\n",
        "    Performs stochastic gradient descent to minimize the loss function of logistic regression.\n",
        "\n",
        "    Args:\n",
        "    - X (ndarray): Input data with shape (n_samples, n_features).\n",
        "    - y (ndarray): Target labels with shape (n_samples,).\n",
        "    - c (ndarray): Initial weight vector with shape (n_features,).\n",
        "    - epochs (int): The number of epochs to train the model.\n",
        "    - learning_rate (float): The learning rate of the model.\n",
        "    - batch_size (int): The batch size to use during training.\n",
        "\n",
        "    Returns:\n",
        "    - c (ndarray): The learned weight vector with shape (n_features,).\n",
        "    - loss_history (list): A list of the loss values during training.\n",
        "    \"\"\"\n",
        "    y = y.reshape(-1, 1)\n",
        "    loss_history = [0]*epochs\n",
        "    for epoch in range(epochs):\n",
        "        # loop through batches\n",
        "        batch_loss = []\n",
        "        for i in np.arange(0, X.shape[0], batch_size):\n",
        "            X_current_batch = X[i:i+batch_size]\n",
        "            y_current_batch = y[i:i+batch_size]\n",
        "            yhat = predictor(X_current_batch, c)\n",
        "            loss_current_batch = loss(X_current_batch, c, y_current_batch).ravel()\n",
        "            batch_loss.append(loss_current_batch)\n",
        "            gradient = XT.dot(yhat - y)/float(len(y))\n",
        "            c = c - learning_rate*gradient\n",
        "        loss_history[epoch] = np.average(batch_loss)\n",
        "    return c, loss_history"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bc0da0bd7c01680fa8348b7a26d6a24694e547ab16f98204be81d32b3a15c474"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
