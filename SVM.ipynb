{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "This notebook contains a from-scratch implementation of a Support Vector Machine (SVM) binary\n",
    "classifier. This implementation only uses basic Python libraries like numpy and pandas, aside from\n",
    "some basic scikit-learn functionality for loading and managing test datasets.\n",
    "\n",
    "Joseph Melby,\n",
    "2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "\n",
    "This supervised learning algorithm constructs a hyperplane in $n$-dimensional space that corresponds\n",
    "to a decision boundary of a binary classification problem. It does this by initializing and then\n",
    "optimizing a set of feature weights and finding an intercept (parameters for a hyperplane). In\n",
    "particular, we begin with a feature matrix $X$ of dimension $n$ and vector $y$ such that\n",
    "\n",
    "- each row $X_i$ of $X$ contains features of the data set, and \n",
    "- each element $y_i$ of $y$ is a positive or negative number giving the feature vector class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperplane\n",
    "The model is optimized to find a set of weights $W$ and an intercept $b$ defining a\n",
    "hyperplane\n",
    "\n",
    "$$W\\cdot X + b$$\n",
    "\n",
    "such that the function \n",
    "\n",
    "$$f(X_i) = sign(W\\cdot X_i + b)$$\n",
    "\n",
    "gives the model's classification of feature vector $X_i$ for each row of $X$. The hyperplane\n",
    "parameters are optimized by both minimizing a cost function and maximizing the margin of separation\n",
    "of the plane.\n",
    "\n",
    "As a convenience for this implementation, we can push the intercept $b$ into the weights $W$ by just\n",
    "adding them in as an extra column to the end of $W$:\n",
    "\n",
    "$$ f(X_i) = \\tilde{W} \\cdot \\tilde{X_i} + W_0 = \\mathbf{W} \\cdot X_i $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ \\mathbf{W} = \\left(\\tilde{W}, W_0 \\right), \\qquad X_i = \\left( \\tilde{X_i}, 1 \\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function\n",
    "\n",
    "We have two options for the cost function, each with parameters that affect the model in slightly\n",
    "different contexts. The first is \n",
    "\n",
    "$$ J(W) = \\frac{1}{2}||W||^2 + C \\left[ \\frac{1}{n} \\sum_{i=1}^{n} max \\left(0, 1 - y_i \\left(W\n",
    "\\cdot X_i\n",
    "+ b \\right)\\right)\\right] $$\n",
    "\n",
    "Here, $C$ is affected while training the model; a larger $C$ corresponds to a narrow margin between\n",
    "the hyperplane and support vector. The second option is \n",
    "\n",
    "$$ J(W) = \\frac{\\lambda}{2}||W||^2 + \\frac{1}{n} \\sum_{i=1}^{n} max(0, 1 - y_i(W \\cdot X_i + b)) $$\n",
    "\n",
    "Here a larger $\\lambda$ gives a wider margin and a smaller $\\lambda$ gives a narrow margin.\n",
    "Basically, we can think of $\\lambda$ as essentially equal to $1/C$. These parameters encode the\n",
    "regularization strength, and they will need to be tuned with the other model parameters. \n",
    "\n",
    "As a final note, once we write the cost function in the implementation below, the combined\n",
    "weight-intercept matrix $\\mathbf{W}$ will be used instead of the explicit formula above:\n",
    "\n",
    "$$ J(\\mathbf{W}) = \\frac{1}{2}||\\mathbf{W}||^2 + C \\left[ \\frac{1}{n} \\sum_{i=1}^{n} max \\left(0, 1\n",
    "- y_i \\left(\\mathbf{W} \\cdot X_i\\right)\\right)\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Calculation\n",
    "\n",
    "The gradient of the cost function is given by the following:\n",
    "\n",
    "$$ \\nabla_{\\mathbf{W}} J(\\mathbf{W}) = \\frac{1}{n} \\sum_{i=1}^n \n",
    "   \\begin{cases}\n",
    "    \\mathbf{W} \\qquad \\text{if } max \\left(0, 1 - y_i \\left(\\mathbf{W} \\cdot X_i\\right)\\right) = 0\\\\\n",
    "    \\mathbf{W} - C y_i x_i \\qquad \\text{otherwise}.\n",
    "   \\end{cases} $$\n",
    "\n",
    "The process for updating weights using gradient descent is the following:\n",
    "\n",
    "1. Compute gradient: $\\nabla_{\\mathbf{W}} J(\\mathbf{W})$\n",
    "2. Update weights by moving opposite the gradient with learning rate $\\alpha$: $\\mathbf{W} =\n",
    "   \\mathbf{W} - \\alpha \\nabla_{\\mathbf{W}} J(\\mathbf{W})$ \n",
    "3. Repeat until, ideally, $J(W)$ is minimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# For splitting test data sets:\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
