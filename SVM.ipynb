{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "This notebook contains a from-scratch implementation of a Support Vector Machine (SVM) binary\n",
    "classifier. This implementation only uses basic Python libraries like numpy and pandas, aside from\n",
    "some basic scikit-learn functionality for loading and managing test datasets.\n",
    "\n",
    "Joseph Melby,\n",
    "2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This supervised learning algorithm constructs a hyperplane in $n$-dimensional space that corresponds\n",
    "to a decision boundary of a binary classification problem. It does this by initializing and then\n",
    "optimizing a set of feature weights and finding an intercept (parameters for a hyperplane). In\n",
    "particular, we begin with a feature matrix $X$ of dimension $n$ and vector $y$ such that\n",
    "\n",
    "- each row $X_i$ of $X$ contains features of the data set, and \n",
    "- each element $y_i$ of $y$ is a positive or negative number giving the feature vector class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperplane Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is optimized to find a set of weights $W$ and an intercept $b$ defining a\n",
    "hyperplane\n",
    "\n",
    "$$W\\cdot X + b$$\n",
    "\n",
    "such that the function \n",
    "\n",
    "$$f(X_i) = sign(W\\cdot self.features_i + b)$$\n",
    "\n",
    "gives the model's classification of feature vector $X_i$ for each row of $X$. The hyperplane\n",
    "parameters are optimized by both minimizing a cost function and maximizing the margin of separation\n",
    "of the plane.\n",
    "\n",
    "As a convenience for this implementation, we can push the intercept $b$ into the weights $W$ by just\n",
    "adding them in as an extra column to the end of $W$:\n",
    "\n",
    "$$ f(X_i) = \\tilde{W} \\cdot \\tilde{X_i} + W_0 = \\mathbf{W} \\cdot X_i $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ \\mathbf{W} = \\left(\\tilde{W}, W_0 \\right), \\qquad X_i = \\left( \\tilde{X_i}, 1 \\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two options for the cost function, each with parameters that affect the model in slightly\n",
    "different contexts. The first is \n",
    "\n",
    "$$ J(W) = \\frac{1}{2}||W||^2 + C \\left[ \\frac{1}{n} \\sum_{i=1}^{n} max \\left(0, 1 - y_i \\left(W\n",
    "\\cdot X_i\n",
    "+ b \\right)\\right)\\right] $$\n",
    "\n",
    "Here, $C$ is affected while training the model; a larger $C$ corresponds to a narrow margin between\n",
    "the hyperplane and support vector. The second option is \n",
    "\n",
    "$$ J(W) = \\frac{\\lambda}{2}||W||^2 + \\frac{1}{n} \\sum_{i=1}^{n} max(0, 1 - y_i(W \\cdot X_i + b)) $$\n",
    "\n",
    "Here a larger $\\lambda$ gives a wider margin and a smaller $\\lambda$ gives a narrow margin.\n",
    "Basically, we can think of $\\lambda$ as essentially equal to $1/C$. These parameters encode the\n",
    "regularization strength, and they will need to be tuned with the other model parameters. \n",
    "\n",
    "As a final note, once we write the cost function in the implementation below, the combined\n",
    "weight-intercept matrix $\\mathbf{W}$ will be used instead of the explicit formula above:\n",
    "\n",
    "$$ J(\\mathbf{W}) = \\frac{1}{2}||\\mathbf{W}||^2 + C \\left[ \\frac{1}{n} \\sum_{i=1}^{n} max \\left(0, 1\n",
    "- y_i \\left(\\mathbf{W} \\cdot X_i\\right)\\right)\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the cost function is given by the following:\n",
    "\n",
    "$$ \\nabla_{\\mathbf{W}} J(\\mathbf{W}) = \\frac{1}{n} \\sum_{i=1}^n \n",
    "   \\begin{cases}\n",
    "    \\mathbf{W} \\qquad \\text{if } max \\left(0, 1 - y_i \\left(\\mathbf{W} \\cdot X_i\\right)\\right) = 0\\\\\n",
    "    \\mathbf{W} - C y_i x_i \\qquad \\text{otherwise}.\n",
    "   \\end{cases} $$\n",
    "\n",
    "The process for updating weights using gradient descent is the following:\n",
    "\n",
    "1. Compute gradient: $\\nabla_{\\mathbf{W}} J(\\mathbf{W})$\n",
    "2. Update weights by moving opposite the gradient with learning rate $\\alpha$: $\\mathbf{W} =\n",
    "   \\mathbf{W} - \\alpha \\nabla_{\\mathbf{W}} J(\\mathbf{W})$ \n",
    "3. Repeat until, ideally, $J(W)$ is minimized.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch vs. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above process can be implemented in multiple ways, most commonly by computing the gradient on\n",
    "either the entire data set at once (known as Batch Gradient Descent (BCG)) or on smaller random\n",
    "samplings of the data set (known as Stochastic Gradient Descent (SGD)). A great description of the\n",
    "differences between the two approaches can be found at \n",
    "\n",
    "https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent\n",
    "\n",
    "but let's give a brief description here.\n",
    "\n",
    "BGD works well for geometrically \"nice\" error manifolds (the space on which we are minimizing the\n",
    "function $J(\\mathbf{W})$). In this case, global minimums are likely computed more efficiently due to\n",
    "the simplicity of the gradient calculation on the error manifold. \n",
    "\n",
    "SGD, on the other hand, tends to work better on more complicated error manifolds. This is because\n",
    "the gradient calculations only occur on a small sampling of the data, and the model is less likely\n",
    "to get \"stuck\" in a local minimum, of which there may be many depending on the geometry of the error\n",
    "manifold. The resulting process tends to also be much more efficient computationally, requiring less\n",
    "memory on the system. \n",
    "\n",
    "BGD's computational expense and \"niceness\" assumptions make it appropriate for getting the idea of\n",
    "the algorithm across, but it is not generally used in practice. SGD benefits from the statistical\n",
    "stability that comes with averaging out repeated random samplings of a dataset and performing\n",
    "simpler calculations in each pass. \n",
    "\n",
    "We will give an implementation of both below just to play around with these differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will also help us to integrate a stoppage condition in order to prevent our SGD procedure from\n",
    "overdoing it. The way we will do it is to first set a percentage improvement on the\n",
    "cost-minimization process every $2^{n}th$ epoch which will be known as the cost threshold. The model\n",
    "will continue to improve until it fails to improve by the cost threshold percentage, at which time\n",
    "it will terminate and return the most up-to-date weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "# For splitting test data sets:\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVM class\n",
    "class SVM():\n",
    "    \"\"\"This class gives an implementation of a support vector machine binary classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, regularization=1000, learning_rate=0.01, \n",
    "                 max_epochs=5000, cost_thresh=0.01):\n",
    "        \"\"\"Initialize basic parameters and run a train-test-split, fit the model, and test on the\n",
    "        test subset of the input data set.\n",
    "\n",
    "        Args:\n",
    "            X (2DArray): Input features (n x m matrix with n data points and m features)\n",
    "            y (1DArray): Output classes (n x 1 matrix of classes associated to data)\n",
    "            regularization (float, optional): regularization constant. Defaults to 1000.\n",
    "            learning_rate (float, optional): learning rate for gradient descent. Defaults to 0.01.\n",
    "            max_epochs (int, optional): number of epochs to iterate through. Defaults to 5000.\n",
    "            cost_thresh (float, optional): Percentage threshold for cost minimization process. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        self.features = X\n",
    "        self.outputs = y\n",
    "        self.regularization = regularization\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.cost_threshold = cost_thresh\n",
    "        self.N = X.shape[0]\n",
    "\n",
    "        # TODO: Add in normalization step for the input data\n",
    "\n",
    "        # Add a column of 1's to the feature data since we combined the intercept calculation with\n",
    "        # the rest of the weights\n",
    "        self.features = np.concatenate((self.features, np.ones((self.shape[0], 1))), axis = 1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "\n",
    "        # Train-Test-split\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test = train_test_split(X, y,\n",
    "                                                                                shuffle=True,\n",
    "                                                                                random_state=435,\n",
    "                                                                                test_size=.25)\n",
    "\n",
    "        # TODO:Add calls to train and test models after the split\n",
    "\n",
    "    def cost(self, weights, features, outputs):\n",
    "        N = features.shape[0]\n",
    "        # distances from the hyperplane\n",
    "        distances = 1 - outputs*(np.dot(weights, features))\n",
    "        # filter out negative distances and convert to 0 (same as max function above)\n",
    "        distances[distances < 0] = 0\n",
    "        sum_loss = self.regularization* (np.sum(distances)/N)\n",
    "\n",
    "        cost = 1 / 2 * np.dot(weights, weights) + sum_loss\n",
    "        return cost\n",
    "\n",
    "    # Define batch gradient, which can be used to compute the gradient on any size batch/sampling\n",
    "    def cost_gradient_batch(self, weights, X_bat, y_bat):\n",
    "        # check if numbers or vectors are given (single item batches)\n",
    "        if type(y_bat) == np.float_:\n",
    "            y_bat = np.array([y_bat])\n",
    "            X_bat = np.array([X_bat])\n",
    "        \n",
    "        # now we can run the same process for any size batch\n",
    "        distance = 1 - (y_bat * np.dot(X_bat, weights))\n",
    "        W_grad = np.zeros(len(weights)) # TODO: if W is passed as an array, may need to switch to shape[0]\n",
    "\n",
    "        for ind, dist in enumerate(distance):\n",
    "            if max(0, dist) == 0:\n",
    "                dist_ind = weights\n",
    "            else:\n",
    "                dist_ind = weights - (self.regularization * y_bat[ind] * X_bat[ind])\n",
    "            W_grad += dist_ind\n",
    "        W_grad = W_grad/len(y_bat)\n",
    "\n",
    "        return W_grad\n",
    "\n",
    "    # Define the stochastic gradient descent function\n",
    "    def sgd(self, features, outputs):\n",
    "        weights = np.zeros(features.shape[1])\n",
    "\n",
    "        # Set up stoppage condition params\n",
    "        n_power = 0\n",
    "        prev_cost = float('inf')\n",
    "\n",
    "        for epoch in range(1, self.max_epochs):\n",
    "            # shuffle data\n",
    "            X_shuff, y_shuff = shuffle(features, outputs)\n",
    "            # loop through and compute gradients for each datum\n",
    "            for ind, x in enumerate(X_shuff):\n",
    "                grad = self.cost_gradient_batch(x, outputs[ind])\n",
    "                weights -= self.learning_rate*grad \n",
    "\n",
    "            # Check to see if we hit our stoppage condition\n",
    "            if epoch == 2**(n_power) or epoch == self.max_epochs - 1:\n",
    "                new_cost = self.cost(weights, features, outputs)\n",
    "                print(\"Epoch is:{} and Cost is: {}\".format(epoch, new_cost))\n",
    "                # check stoppage\n",
    "                if abs(prev_cost - new_cost) < self.cost_threshold * prev_cost:\n",
    "                    return weights\n",
    "                prev_cost = new_cost\n",
    "                n_power += 1\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
